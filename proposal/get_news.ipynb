{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd   # to process data\n",
    "import numpy as np\n",
    "import requests     # to fetch the data from the api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define common variables\n",
    "\n",
    "#  link is the api link which will return the data\n",
    "# q is the query keyword to search for .i.e apple and microsoft\n",
    "# end and start date will be used to specify the range\n",
    "link = \"http://api.nytimes.com/svc/search/v2/articlesearch.json?q={}&begin_date={}&end_date={}&api-key=ce622601a6b047dc88b57aff2a04e921\"\n",
    "final_link=''\n",
    "blank_count=0    # var to count the number of blank returns fromt he api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the microsoft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the csv files to have a basic data frame to work with\n",
    "micronews= pd.read_csv('MSFT.csv')   # a data frame to add news\n",
    "micronews['News']='' # add a column to store the news \n",
    "\n",
    "microsenti= pd.read_csv('MSFT.csv')  # data frame to store the sentiment analysis\n",
    "microsenti[\"compound\"] = float      # extra columns to store the sentiment score\n",
    "microsenti[\"neg\"] = float\n",
    "microsenti[\"neu\"] = float\n",
    "microsenti[\"pos\"] = float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blank_count=0\n",
    "for row in range(0,len(micronews)):       # change the range values to refetch the data specific fields\n",
    "    # get the date to fetch the data from the data frame\n",
    "    day=str(micronews['Date'][row].replace('-',''))\n",
    "    # add this data into the final link\n",
    "    final_link= link.format('microsoft',day,str(int(day)+1))\n",
    "    # get the data from the api\n",
    "    r = requests.get(final_link)\n",
    "    data = r.json()  \n",
    "    \n",
    "    news=''   # blank string\n",
    "    # if there is news on that given date range than the api return will contain the 'response' as one of the key\n",
    "    if 'response' in data.keys():\n",
    "        for i in range(len(data['response']['docs'])):\n",
    "            news+=str(data['response']['docs'][i]['headline']['main'])+' : '+str(data['response']['docs'][i]['snippet'])\n",
    "        micronews['News'][row]=news    \n",
    "        blank_count = 0\n",
    "    \n",
    "    # for the case when there is no data about that that topic\n",
    "    else:\n",
    "        print(row,data)\n",
    "        blank_count += 1\n",
    "    # if it doesnot give data for 50 contineous dates, than this implies that there is some problem with the api or the api key\n",
    "    if blank_count >= 50: \n",
    "        print(row,data)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the data into a csv file\n",
    "micronews.to_csv('MicrosoftNews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now use the abone news dataframe to find its sentiment score\n",
    "# few extra headers\n",
    "# from nltk.classify import NaiveBayesClassifier\n",
    "# from nltk.corpus import subjectivity\n",
    "# from nltk.sentiment import SentimentAnalyzer\n",
    "# from nltk.sentiment.util import *\n",
    "\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import unicodedata\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "for date, row in micronews.T.iteritems():\n",
    "    try:\n",
    "        ss = sid.polarity_scores( micronews.loc[date, 'News'])\n",
    "        microsenti.set_value(date, 'compound', ss['compound'])\n",
    "        microsenti.set_value(date, 'neg', ss['neg'])\n",
    "        microsenti.set_value(date, 'neu', ss['neu'])\n",
    "        microsenti.set_value(date, 'pos', ss['pos'])\n",
    "    except TypeError:\n",
    "        print (micronews.loc[date, 'articles'])\n",
    "        print (date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final data processing\n",
    "microsenti=microsenti.round(4)    \n",
    "microsenti=microsenti.drop(['Volume'],axis=1)    # remove the unused field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the final data set created\n",
    "microsenti.to_csv('MicrosoftFinalData.csv')\n",
    "\n",
    "# this complete process will be perfomed with the apple data set also"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
